# NaverAIBoostCamp_7기_paper [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

🔥25/02/21자로 **모든 paper를 추가**하였습니다!🔥

[![Since](https://img.shields.io/badge/since-2025.01.01-333333.svg?style=flat-square)](https://github.com/2JAE22/AI_Tech-paper)
[![author](https://img.shields.io/badge/author-2JAE22-0066FF.svg?style=flat-square)](https://github.com/2JAE22)
[![LICENSE](https://img.shields.io/github/license/2JAE22/AI_Tech-paper.svg?style=flat-square)](https://github.com/2JAE22/AI_Tech-paper/blob/main/LICENSE)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2F2JAE22%2FAI_Tech-paper%2Fhit-counter&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)
[![All Contributors](https://img.shields.io/badge/all_contributors-2?style=flat-square)](https://github.com/2JAE22/AI_Tech-paper/graphs/contributors)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-FF66FF.svg?style=flat-square)](http://makeapullrequest.com)

[![Watch on GitHub](https://img.shields.io/github/watchers/2JAE22/AI_Tech-paper.svg?style=social)](https://github.com/2JAE22/AI_Tech-paper/watchers)
[![Star on GitHub](https://img.shields.io/github/stars/2JAE22/AI_Tech-paper.svg?style=social)](https://github.com/2JAE22/AI_Tech-paper/stargazers)
[![Fork on GitHub](https://img.shields.io/github/forks/2JAE22/AI_Tech-paper.svg?style=social)](https://github.com/2JAE22/AI_Tech-paper/network/members)
<br>

<br>

### 🚀 NaverAIBoostCamp 논문 및 further reading모음 📖

<br> 

<br> 

**Collaborator**

[<img src="https://avatars.githubusercontent.com/u/87936538?v=4" width="100">](https://github.com/2JAE22)  
[GitHub](https://github.com/2JAE22)

<br>

**Commit convention rule** : 날짜-[주제]-내용-상태

`ex) 2025-01-04 [CV Recent Trends] All/Only_Paper/Only_Further Add/Update/Delete`

<br>

잘못된 내용은 [이슈](https://github.com/2JAE22/AI_Tech-paper/issues)와 [PR](https://github.com/2JAE22/AI_Tech-paper/pulls)로 알려주세요 💡

<br>



<center>🙏도움을 주신 분들🙏</center>

<br>
<br>

<a href="https://github.com/2JAE22/AI_Tech-paper/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=2JAE22/AI_Tech-paper" />
</a>

<br>


#### 더 좋은 컨텐츠를 제공받길 원한다면 [💖후원하기!!💝](https://github.com/sponsors/2JAE22)

# 1. 📌Introduction
NaverAIboostCamp에서 소개한 논문들을 주제별로 정리한 폴더입니다.
현재는 CV(Computer Vision) 트랙의 논문을 중심으로 정리하고 있으며, 추후 모든 트랙으로 확장할 예정입니다.

# 2에서 5까지는 논문만 있습니다.
- [6](#6-further-reading에-있었던-것들)부터 논문외의 사이트들을 정리하였습니다.

# 2. 📌CV 트랙 정리(Only Paper)
## 2.1 CV 이론
- [VGGNet](https://arxiv.org/abs/1409.1556)  
- [ResNet](https://arxiv.org/abs/1512.03385)  
- [ViT](https://arxiv.org/abs/2010.11929)
- [Grad-CAM](https://arxiv.org/abs/1610.02391)
- [mixup](https://arxiv.org/abs/1710.09412)
- [CutMix](https://arxiv.org/abs/1905.04899)
- [Fully Convolutional Networks for Semantic Segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)
- [SAM](https://arxiv.org/pdf/2304.02643)
- [DETR](https://arxiv.org/pdf/2005.12872)
- [Real-World Single Image Super-Resolution: A New Benchmark and A New Model](https://arxiv.org/abs/1904.00523)
- [Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700188.pdf)
- [Blind Super-Resolution Kernel Estimation using an Internal-GAN](https://arxiv.org/abs/1909.06581)
- [SpatialTracker: Tracking Any 2D Pixels in 3D Space](https://arxiv.org/abs/2404.04319)
- [CLIP huggingface implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py)
- [ImageBIND official implementation](https://github.com/facebookresearch/ImageBind)
- [LanguageBIND](https://arxiv.org/abs/2310.01852)
- [Flamingo pytorch implementation](https://github.com/lucidrains/flamingo-pytorch/blob/main/flamingo_pytorch/flamingo_pytorch.py)
- [LLaVA](https://llava-vl.github.io/)
- [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)
- [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
- [DDPM](https://arxiv.org/abs/2006.11239)
- [LDM (Stable Diffusion)](https://arxiv.org/abs/2112.10752)
- [DDIM](https://arxiv.org/abs/2010.02502) 
- [3D MACHINE LEARNING](https://www.antoinetlc.com/blog-summary/3d-data-representations)
- [Mesh R-CNN](https://arxiv.org/abs/1906.02739)
- [NeRF](https://arxiv.org/abs/2003.08934)
- [3DGS](https://arxiv.org/abs/2308.04079)
- [DreamFusion](https://arxiv.org/abs/2209.14988)
- [Loper et al., SMPL: A Skinned Multi-Person Linear Model: SIGGRAPH 2015.](https://dl.acm.org/doi/10.1145/2816795.2818013)
- [Bogo et al., Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image: ECCV 2016.](https://arxiv.org/abs/1607.08128)
- [Anguelov et al., SCAPE: Shape Completion and Animation of People: SIGGRAPH 2005.](https://dl.acm.org/doi/10.1145/1073204.1073207)


## 2.2 CV 기초 프로젝트
- [A survey on Image Data Augmentation for Deep Learning](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)
- [AutoAugment: Learning Augmentation Strategies from Data](https://arxiv.org/abs/1805.09501)
- [RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/abs/1909.13719)
- [Fine-Grained Image Analysis with Deep Learning: A Survey](https://arxiv.org/abs/2111.06119)
- [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- [CoAtNet: Marrying Convolution and Attention for All Data Sizes](https://arxiv.org/abs/2106.04803)
- [ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)
- [Multimodal Learning with Transformers: A Survey](https://arxiv.org/abs/2206.06488)
- [Self-supervised Learning: Generative or Contrastive](https://arxiv.org/abs/2006.08218)
- [Ensemble deep learning: A review](https://arxiv.org/abs/2104.02395)
  
## 2.3 Object Detection
- [R-CNN](https://arxiv.org/abs/1311.2524)
- [Fast R-CNN](https://arxiv.org/abs/1504.08083)
- [Faster R-CNN](https://arxiv.org/abs/1506.01497)
- [SPPNet](https://arxiv.org/abs/1406.4729)
- [FPN](https://arxiv.org/abs/1612.03144)
- [PAFPN](https://arxiv.org/abs/1803.01534)
- [DetectoRS](https://arxiv.org/abs/2006.02334)
- [EfficientDet (BiFPN)](https://arxiv.org/abs/1911.09070v7)
- [NasFPN](https://arxiv.org/abs/1904.07392)
- [AugFPN](https://arxiv.org/abs/1912.05384)
- [YOLO survey](https://arxiv.org/abs/2304.00501)
- [Retinanet (focal loss)](https://arxiv.org/abs/1708.02002)
- [SSD](https://arxiv.org/abs/1512.02325)
- [EfficientNet](https://arxiv.org/abs/1905.11946)
- [EfficientDet](https://arxiv.org/abs/1911.09070)
- [DCN](https://arxiv.org/abs/1703.06211)
- [DETR](https://arxiv.org/abs/2005.12872)
- [Swin](https://arxiv.org/abs/2103.14030)
- [YOLO v4](https://arxiv.org/abs/2004.10934)
- [M2Det](https://arxiv.org/abs/1811.04533)
- [CornerNet](https://arxiv.org/abs/1808.01244)

## 2.4 Data-Centric AI
- [DMLR](https://openreview.net/pdf?id=2kpu78QdeE)
- [Convolutional Character Networks](https://openaccess.thecvf.com/content_ICCV_2019/papers/Xing_Convolutional_Character_Networks_ICCV_2019_paper.pdf)
- [EAST](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf)
- [Data and its (dis)contents: A survey of dataset development and use in machine learning research](https://www.sciencedirect.com/science/article/pii/S2666389921001847)
- [Human-In-The-Loop에 대한 survey 논문](https://www.sciencedirect.com/science/article/abs/pii/S0167739X22001790?casa_token=5poWCKizHjIAAAAA:Z8eK3GMWCCwOncUmdz2J8JHGNYAx3N4MW_31Uq3CnWVQN2C6RXXtOqc50GveYglcudc9TiwhYKk)
- [다양한 task에 적용 가능한 IAA에 관한 논문](https://dl.acm.org/doi/10.1145/3485447.3512242)
- [LLM을 활용한 data annotation에 관한 survey 논문](https://arxiv.org/abs/2402.13446)
- [A survey on Image Data Augmentation for Deep Learning](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)
- [A survey of synthetic data augmentation methods in computer vision](https://arxiv.org/abs/2403.10075)



## 2.5 Semantic Segmentation
- [FCN](https://arxiv.org/abs/1411.4038)
- [DeconvNet](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf)
- [SegNet](https://arxiv.org/pdf/1511.00561)
- [FCDenseNet](https://arxiv.org/pdf/1611.09326)
- [Unet](https://arxiv.org/abs/1505.04597)
- [DeepLabv1](https://arxiv.org/pdf/1412.7062)
- [DilatedNet ](https://arxiv.org/abs/1511.07122)
- [DeepLabv2](https://arxiv.org/pdf/1606.00915)
- [PSPNet](https://arxiv.org/pdf/1612.01105)
- [DeepLabv3](https://arxiv.org/pdf/1706.05587)
- [DeepLabv3+](https://arxiv.org/pdf/1802.02611)
- [Unet](https://arxiv.org/abs/1505.04597)
- [Unet++](https://arxiv.org/pdf/1912.05074)
- [Unet3+](https://arxiv.org/abs/2004.08790)
- [EfficientUnet](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w22/Baheti_Eff-UNet_A_Novel_Architecture_for_Semantic_Segmentation_in_Unstructured_Environment_CVPRW_2020_paper.pdf)
- [DenseUnet](https://arxiv.org/abs/1611.09326)
- [ResidualUnet](https://arxiv.org/pdf/1711.10684)
- [SWA](https://arxiv.org/abs/1803.05407)
- [HRNet](https://arxiv.org/pdf/1908.07919)
- [SegFormer](https://arxiv.org/pdf/2105.15203)
- [ViT](https://arxiv.org/pdf/2010.11929)
- [Weakly Supervised Object Localization and Detection: A Survey](https://arxiv.org/abs/2104.07918)

## 2.6 CV Recent Trends
- [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)
- [Variational Diffusion Models](https://arxiv.org/abs/2107.00630)
- [NVAE: A Deep Hierarchical Variational Autoencoder](https://arxiv.org/abs/2007.03898)
- [Denoising Diffusion Probabilistic Model](https://arxiv.org/abs/2006.11239)
- [Improved Denoising Diffusion Probabilistic Model](https://arxiv.org/abs/2102.09672)
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)
- [Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/abs/2202.00512)
- [Consistency Models](https://arxiv.org/abs/2303.01469)
- [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233)
- [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)
- [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487)
- [Hierarchical Text-Conditional Image Generation with CLIP latents](https://arxiv.org/abs/2204.06125)
- [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://arxiv.org/abs/2108.01073)
- [Dual Diffusion Implicit Bridges for Image-to-Image Translation](https://arxiv.org/abs/2203.08382)
- [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)
- [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/abs/2208.01618)
- [Dreambooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242)
- [Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2303.07345)
- [Unified Concept Editing in Diffusion Models](https://arxiv.org/abs/2308.14761)
- [Video Diffusion Models](https://arxiv.org/abs/2204.03458)
- [Video Probabilistic Diffusion Models in Projected Latent Space](https://arxiv.org/abs/2302.07685)
- [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2304.08818)
- [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988)
- [zero-1-to-3: zero-shot one image to 3d object](https://arxiv.org/abs/2303.11328)
- [Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models](https://arxiv.org/abs/2310.03020)
- [Cascaded Diffusion Models for High Fidelity Image Generation](https://arxiv.org/abs/2106.15282)
- [DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior](https://arxiv.org/abs/2308.15070)
- [Solving Inverse Problem in Medical Imaging with Score-Based Generative Models](https://arxiv.org/abs/2111.08005)
- [Label-Efficient Semantic Segmentation with Diffusion Models](https://arxiv.org/abs/2112.03126)
- [ODISE: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models](https://arxiv.org/abs/2303.04803)
- [Your Diffusion Model is secretly a zero-shot classifier](https://arxiv.org/abs/2303.16203)
- [Emergent Correspondence from Image Diffusion](https://arxiv.org/abs/2306.03881)
- [AnoDDPM: Anomaly Detection with Denoising Diffusion Probabilistic Models using Simplex Noise](https://ieeexplore.ieee.org/document/9857019)


# 3. 📌NLP 트랙 정리(Only Paper)
## 3.1 NLP 이론
- [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
- [GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/)
- [LSTM](https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext)
- [Highway Networks](https://arxiv.org/abs/1505.00387)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
- [Sparse is Enough in Scaling Transformers](https://openreview.net/pdf?id=-b5OSCydOMe)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Layer Normalization](https://arxiv.org/abs/1607.06450)
- [Group Normalization](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf)
- [Attention is not Explanation](https://arxiv.org/pdf/1902.10186)
- [Attention is not not Explanation](https://aclanthology.org/D19-1002.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
- [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)
- [Neural Network Acceptability Judgments](https://arxiv.org/abs/1805.12471)
- [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](https://arxiv.org/abs/1704.05426)
- [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250)
- [Hierarchical Neural Story Generation](https://arxiv.org/abs/1805.04833)
- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)

  
## 3.2 NLP 기초 프로젝트
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Quantifying Attention Flow in Transformers](https://arxiv.org/abs/2005.00928)
- [LoRA](https://arxiv.org/abs/2106.09685)
- [Non-Autoregressive & Autoregressive](https://arxiv.org/abs/2102.08220)


## 3.3 MRC(Machine Reading Comprehension)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.boostcourse.org/boostcampaitech7/lecture/1545463?isDesc=false)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5) ](https://arxiv.org/abs/1910.10683)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)
- [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051)
- [A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets](https://arxiv.org/abs/2006.11880)
- [Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/abs/1906.00300)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)
- [Exploring the limits of transfer learning with a unified text-to-text transformer(T5)](https://arxiv.org/abs/1910.10683)
- [How much knowledge can you pack into the parameters of language model?](https://arxiv.org/abs/2002.08910)
- [UNIFIEDQA: Crossing Format Boundaries with a Single QA System](https://arxiv.org/abs/2005.00700)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index](https://arxiv.org/abs/1906.05807)
- [Contextualized Sparse Representations for Real-Time Open-Domain Question Answering](https://arxiv.org/abs/1911.02896)


## 3.4 Data-Centric NLP
- [DMOps: Data Management Operation and Recipes](https://arxiv.org/abs/2301.01228)
- [A Survey on Awesome Korean NLP Datasets](https://ieeexplore.ieee.org/abstract/document/9952930)
- [ProsocialDialog: A Prosocial Backbone for Conversational Agents](https://aclanthology.org/2022.emnlp-main.267/)
- [Natural Language Processing: State of The Art, Current Trends and Challenges](https://arxiv.org/abs/1708.05148)
- [Understanding Back-Translation at Scale](https://arxiv.org/abs/1808.09381)
- [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://arxiv.org/abs/1901.11196)
- [Data Augmentation using Pre-trained Transformer Models](https://arxiv.org/abs/2003.02245)
- [AugGPT: Leveraging ChatGPT for Text Data Augmentation](https://arxiv.org/abs/2302.13007)
- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/abs/2107.13586)
- [Everyone's Voice Matters: Quantifying Annotation Disagreement Using Demographic Information](https://arxiv.org/abs/2301.05036)
- [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)


## 3.5 Generative for NLP
- [Emergent Abilities](https://arxiv.org/pdf/2206.07682)
- [Few-shot Learning (GPT-3)](https://arxiv.org/pdf/2005.14165)
- [Chain of Thought](https://arxiv.org/pdf/2201.11903)
- [Flash Attention 2](https://arxiv.org/abs/2307.08691)
- [RoPE](https://arxiv.org/pdf/2104.09864)
- [A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification](https://arxiv.org/abs/1510.03820)
- [Scaling laws for neural language models](https://arxiv.org/abs/2001.08361)
- [Training compute-optimal large language models](https://arxiv.org/abs/2203.15556)
- [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971)
- [SuperNLI: Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks](https://arxiv.org/abs/2204.07705)
- [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)
- [GPT-3: Language models are few-shot learners.](https://arxiv.org/abs/2005.14165)
- [Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation](https://arxiv.org/abs/2305.16938)
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
- [Learning to summarize with human feedback](https://arxiv.org/abs/2009.01325)
- [PPO: Proximal policy optimization algorithms](https://arxiv.org/abs/1707.06347)
- [DPO: Direct preference optimization: Your language model is secretly a reward model](https://arxiv.org/abs/2305.18290)
- [ORPO: Reference-free monolithic preference optimization with odds ratio](https://arxiv.org/abs/2403.07691)
- [The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only.](https://arxiv.org/abs/2306.01116)
- [The curious case of neural text degeneration](https://arxiv.org/abs/1904.09751)
- [The llama 3 herd of models](https://arxiv.org/abs/2407.21783)
- [Self-instruct: Aligning language models with self-generated instructions](https://arxiv.org/abs/2212.10560)
- [A Survey on Evaluation of Large Language Models](https://arxiv.org/pdf/2307.03109)
- [LLM의 위치 편향](https://arxiv.org/pdf/2307.03172)
- [WildBench 논문](https://arxiv.org/abs/2406.04770)
- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)
- [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/abs/2306.15595)
- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)
- [Ring Attention with Blockwise Transformers for Near-Infinite Context](https://arxiv.org/abs/2310.01889)
- [RAG의 시작](https://arxiv.org/pdf/2005.11401)
- [Contrastive Learning에 대하여](https://arxiv.org/pdf/2104.08821)
- [LLM 기반 임베딩 모델](https://arxiv.org/pdf/2401.00368)
- [GraphRAG](https://arxiv.org/pdf/2404.16130)
- [Reflexion](https://arxiv.org/pdf/2303.11366)
- [ReAct](https://arxiv.org/abs/2210.03629)
- [Flashattention: Fast and memory-efficient exact attention with io-awareness](https://arxiv.org/abs/2205.14135)
- [Flashattention-2: Faster attention with better parallelism and work partitioning](https://arxiv.org/abs/2307.08691)
- [Fast inference from transformers via speculative decoding](https://arxiv.org/abs/2211.17192)
- [Efficient memory management for large language model serving with pagedattention](https://arxiv.org/abs/2309.06180)

## 3.6 NLP Recent Trends
- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
- [Tree-of-Thoughts ](https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf)
- [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs (Qin et al., ICLR 2024)](https://arxiv.org/abs/2307.16789)
- [Describe, Explain, Plan, and Select (Wang et al., NeurIPS 2023)](https://arxiv.org/abs/2302.01560)
- [Generative Agents: Interactive Simulacra of Human Behavior (Park et al., UIST 2023)](https://arxiv.org/abs/2304.03442)
- [The Rise and Potential of Large Language Model Based Agents: A Survey (Xi et al., arXiv 2023)](https://arxiv.org/abs/2309.07864)
- [Survey of Hallucination in Natural Language Generation (Ji et al., arXiv 2022)](https://arxiv.org/abs/2202.03629)
- [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models (Zhang et al., arXiv 2023)](https://arxiv.org/abs/2309.01219)
- [Bias and Fairness in Large Language Models: A Survey (Gallegos, arXiv 2023)](https://arxiv.org/abs/2309.00770)
- [Toxicity in ChatGPT: Analyzing Persona-assigned Language Models (Deshpande et al., EMNLP 2023)](https://arxiv.org/abs/2304.05335)
- [ACL 2023 Tutorial: Retrieval-based Language Models and Applications](https://aclanthology.org/2023.acl-tutorials.6/)
- [Prompting GPT-3 To Be Reliable (Si et al., ICLR 2023)](https://openreview.net/forum?id=98p5x51L5af)
- [TemporalWiki (Jang et al., EMNLP 2022)](https://aclanthology.org/2022.emnlp-main.418/)
- [Retrieval-Augmented Black-Box Language Models (Shi et al., arXiv 2023)](https://arxiv.org/abs/2301.12652)
- [Rethinking with Retrieval: Faithful Large Language Model Inference (He et al., arXiv 2022)](https://arxiv.org/abs/2301.00303)

# 4. 📌Recsys 트랙 정리(Only Paper)
## 4.1 Recsys 이론
- [Variational Autoencoders for Collaborative Filtering](https://arxiv.org/abs/1802.05814)
- [Diffusion Recommendation Model](https://arxiv.org/abs/2304.04971)
- [Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo](https://www.cs.cmu.edu/~rsalakhu/papers/bpmf.pdf)
- [Probabilistic Matrix Factorization](https://proceedings.neurips.cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf)
- [Understanding Black-box Predictions via Influence Functions, ICML 2017](https://arxiv.org/pdf/1703.04730)
- [Data Shapley: Equitable Valuation of Data for Machine Learning,ICML 2019](https://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf)
- [Data Valuation using Reinforcement Learning, ICML 2020](https://proceedings.mlr.press/v119/yoon20a/yoon20a.pdf)
- [Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value, ICML 2023](https://arxiv.org/pdf/2304.07718)
- [Efficient Neural Causal Discovery without Acyclicity Constraints](https://arxiv.org/abs/2107.10483)
- [A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms ](https://arxiv.org/abs/1901.10912)
- [Causal Reasoning and Large Language Models: Opening a New Frontier for Causality](https://arxiv.org/abs/2305.00050)


## 4.2 ML 기초 프로젝트
- [A Brief Introduction to Machine Learning for Engineers](https://arxiv.org/abs/1709.02840)
- [Zheng, Alice, and Amanda Casari. Feature engineering for machine learning: principles and techniques for data scientists. " O'Reilly Media, Inc.", 2018.](https://books.google.co.kr/books?hl=ko&lr=&id=sthSDwAAQBAJ&oi=fnd&pg=PT14&dq=Zheng,+Alice,+and+Amanda+Casari.+Feature+engineering+for+machine+learning:+principles+and+techniques+for+data+scientists.+%22+O%27Reilly+Media,+Inc.%22,+2018.&ots=ZPZfvU0jz-&sig=tjYsynIo-QTjNiZuFPOuuHB6naY#v=onepage&q=Zheng%2C%20Alice%2C%20and%20Amanda%20Casari.%20Feature%20engineering%20for%20machine%20learning%3A%20principles%20and%20techniques%20for%20data%20scientists.%20%22%20O'Reilly%20Media%2C%20Inc.%22%2C%202018.&f=false)
- [Hastie, Trevor, et al. The elements of statistical learning: data mining, inference, and prediction. Vol. 2. New York: springer, 2009.](https://link.springer.com/book/10.1007/978-0-387-21606-5)
- [SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, 321-357.](https://arxiv.org/abs/1106.1813)
- [Gradient Boosting Paper](https://www.semanticscholar.org/paper/Greedy-function-approximation%3A-A-gradient-boosting-Friedman/1679beddda3a183714d380e944fe6bf586c083cd)
- [XGBoost Paper](https://www.semanticscholar.org/paper/XGBoost%3A-A-Scalable-Tree-Boosting-System-Chen-Guestrin/26bc9195c6343e4d7f434dd65b4ad67efe2be27a)
- [LightGBM Paper](https://www.semanticscholar.org/paper/LightGBM%3A-A-Highly-Efficient-Gradient-Boosting-Tree-Ke-Meng/497e4b08279d69513e4d2313a7fd9a55dfb73273)
- [Lasso Regression Paper](https://webdoc.agsci.colostate.edu/koontz/arec-econ535/papers/Tibshirani%20(JRSS-B%201996).pdf)
- [Ridge Regression Paper](https://mineracaodedados.wordpress.com/wp-content/uploads/2015/06/ridge-regression-biased-estimation-for-nonorthogonal-problems.pdf)
- [Ensemble deep learning: A review](https://arxiv.org/abs/2104.02395)
- [Random Search for Hyper-Parameter Optimization](https://www.jmlr.org/papers/v13/bergstra12a.html)
- [Practical Bayesian Optimization of Machine Learning Algorithms](https://arxiv.org/abs/1206.2944)
- [Hyperparameters and Tuning Strategies for Random Forest](https://arxiv.org/abs/1804.03515)
- [Management of Machine Learning Lifecycle Artifacts: A Survey](https://arxiv.org/abs/2210.11831)


## 4.3 Competitive DS
- [ArcFace: Additive Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/1801.07698)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [LONG SHORT-TERM MEMORY](https://www.bioinf.jku.at/publications/older/2604.pdf)
- [SAINT Model Paper](https://arxiv.org/abs/2108.12775)
- [Deep Knowledge Tracing](https://arxiv.org/abs/1506.05908)


## 4.4 RecSys 기초프로젝트
- [Improved Apriori Algorithm for Mining Association Rules](https://www.mecs-press.org/ijitcs/ijitcs-v6-n7/IJITCS-V6-N7-3.pdf)
- [Matrix Factorization Techniques for Recommender Systems ](https://datajobs.com/data-science-repo/Recommender-Systems-%5bNetflix%5d.pdf)
- [BPR: Bayesian personalized ranking from implicit feedback](https://arxiv.org/pdf/1205.2618)
- [Neural Collaborative Filtering](https://arxiv.org/pdf/1708.05031)
- [Deep Neural Networks for YouTube Recommendations](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf)
- [AutoRec: Autoencoders Meet Collaborative Filtering](https://users.cecs.anu.edu.au/%7Eakmenon/papers/autorec/autorec-paper.pdf)
- [Collaborative Denoising Auto-Encoders for Top-N Recommender Systems](https://www.datascienceassn.org/sites/default/files/Collaborative%20Denoising%20Auto-Encoders%20for%20Top-N%20Recommender%20Systems.pdf)
- [Neural Graph Collaborative Filtering](https://arxiv.org/pdf/1905.08108)
- [LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation](https://arxiv.org/pdf/2002.02126)
- [When Recurrent Neural Networks meet the Neighborhood for Session-Based Recommendation](https://web-ainf.aau.at/pub/jannach/files/Conference_RecSys_2017.pdf)
- [Factorization Machines](https://sdcast.ksdaemon.ru/wp-content/uploads/2020/02/Rendle2010FM.pdf)
- [Field-aware Factorization Machines for CTR Prediction](https://www.csie.ntu.edu.tw/%7Ecjlin/papers/ffm.pdf)
- [Greedy Function Approximation: A Gradient Boosting Machine](https://jerryfriedman.su.domains/ftp/trebst.pdf)
- [XGBoost: A Scalable Tree Boosting System](https://dl.acm.org/doi/pdf/10.1145/2939672.2939785)
- [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)
- [CatBoost: unbiased boosting with categorical features](https://arxiv.org/pdf/1706.09516)
- [Wide & Deep Learning for Recommender Systems](https://dl.acm.org/doi/pdf/10.1145/2988450.2988454)
- [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](https://arxiv.org/pdf/1703.04247)
- [Deep Interest Network for Click-Through Rate Prediction](https://arxiv.org/pdf/1706.06978)
- [Behavior Sequence Transformer for E-commerce Recommendation in Alibaba](https://arxiv.org/pdf/1905.06874)
- [An Empirical Evaluation of Thompson Sampling](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf)
- [A Contextual-Bandit Approach to Personalized News Article Recommendation](https://arxiv.org/pdf/1003.0146)
- [RecSys Challenge 2023: Deep Funnel Optimization with a Focus on User Privacy](https://dl.acm.org/doi/proceedings/10.1145/3626221)

## 4.5 Movie Rec
- [A survey of collaborative filtering techniques](https://dl.acm.org/doi/10.1155/2009/421425)
- [Restricted Boltzmann Machines for Collaborative Filtering](https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf)
- [AutoRec: Autoencoders Meet Collaborative Filtering](https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf)
- [Neural Collaborative Filtering](https://arxiv.org/pdf/1708.05031)
- [Variational Autoencoders for Collaborative Filtering](https://arxiv.org/pdf/1802.05814)
- [Wide & Deep Model](https://arxiv.org/pdf/1606.07792)
- [DeepFM](https://arxiv.org/pdf/1703.04247)
- [Latent Cross](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/46488.pdf)
- [Temporal-Contextual Recommendation in Real-Time](https://openreview.net/pdf?id=8QFKbygVy4r)
- [SASRec](https://arxiv.org/abs/1808.09781)
- [BERT4Rec](https://arxiv.org/abs/1904.06690)
- [Sequential Recommender Systems: Challenges, Progress and Prospects](https://www.ijcai.org/Proceedings/2019/0883.pdf)
- [A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models](https://ui.adsabs.harvard.edu/abs/2021arXiv210713045D/abstract)


## 4.6 Recsys Recent Trends
- [Neural Graph Collaborative Filtering ](https://arxiv.org/pdf/1905.08108)
- [Simplifying and Powering Graph Convolution Network for Recommendation](https://arxiv.org/pdf/2002.02126)
- [Disentangled Graph Collaborative Filtering](https://arxiv.org/pdf/2007.01764)
- [Self-supervised Graph Learning for Recommendation](https://arxiv.org/pdf/2010.10783)
- [Hypergraph Contrastive Collaborative Filtering](https://arxiv.org/pdf/2204.12200)
- [Semi-supervised classification with graph convolutional networks](https://arxiv.org/pdf/1609.02907)
- [Simplifying Graph Convolutional Networks](https://arxiv.org/pdf/1902.07153)
- [Hypergraph Neural Networks](https://arxiv.org/pdf/1809.09401)
- [ A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions](https://dl.acm.org/doi/full/10.1145/3568022)
- [ Self-Attentive Sequential Recommendation](https://arxiv.org/pdf/1808.09781)
- [Sequential Recommendation with Bidirectional Encoder Representations from Transformer](https://arxiv.org/pdf/1904.06690)
- [Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation](https://www.microsoft.com/en-us/research/uploads/prod/2021/05/LighterandBetter_Low-RankDecomposedSelf-AttentionNetworksforNext-ItemRecommendation.pdf)
- [Feature-level Deeper Self-Attention Network for Sequential Recommendation ](https://www.ijcai.org/proceedings/2019/0600.pdf)
- [Noninvasive Self-attention for Side Information Fusion in Sequential Recommendation](https://arxiv.org/pdf/2103.03578)
- [Decoupled Side Information Fusion for Sequential Recommendation](https://arxiv.org/pdf/2204.11046)
- [Sequential Modeling with Multiple Attributes for Watchlist Recommendation in E-Commerce](https://arxiv.org/pdf/2110.11072)
- [DropoutNet: Addressing Cold Start in Recommender Systems](https://papers.nips.cc/paper_files/paper/2017/file/dbd22ba3bd0df8f385bdac3e9f8be207-Paper.pdf)
- [Recommendation for New Users and New Items via Randomized
Training and Mixture-of-Experts Transformation](https://dl.acm.org/doi/pdf/10.1145/3397271.3401178)
- [Variational Autoencoders for Collaborative Filtering (Liang et al., WWW 2018)](https://arxiv.org/pdf/1802.05814)
- [Local Latent Space Models for Top-N Recommendation (Christakopoulou et al., KDD 2018)](https://dl.acm.org/doi/pdf/10.1145/3219819.3220112)
- [TALLRec: An Effective and Efficient Tuning Framework to Align
Large Language Model with Recommendation](https://arxiv.org/pdf/2305.00447)


# 5. 📌공통 강의에서 소개한 논문 정리
## 5.1 Generative AI
- [LLM Survey 논문 (2023)](https://arxiv.org/abs/2303.18223)
- [GAN Survey 논문 (2020)](https://arxiv.org/abs/1906.01529)
- [Diffusion Models Survey 논문 (2024)](https://arxiv.org/abs/2209.00796)
- [RLHF 제안 논문](https://arxiv.org/abs/2203.02155)
- [Large Language Model 서베이 논문](https://arxiv.org/abs/2303.18223)
- [GPT-3 논문](https://arxiv.org/abs/2005.14165)
- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
- [Self-Instruct 논문](https://arxiv.org/abs/2212.10560)
- [Self-Rewarding 논문](https://arxiv.org/pdf/2401.10020)
- [GAN 논문](https://arxiv.org/abs/1406.2661)
- [cGAN 논문](https://arxiv.org/abs/1411.1784)
- [Pix2Pix 논문](https://arxiv.org/abs/1611.07004)
- [CycleGAN 논문](https://arxiv.org/abs/1703.10593)
- [StarGAN 논문](https://arxiv.org/abs/1711.09020)
- [ProgressiveGAN 논문](https://arxiv.org/abs/1710.10196)
- [StyleGAN 논문](https://arxiv.org/abs/1812.04948)
- [VAE 논문](https://arxiv.org/abs/1312.6114)
- [VQ-VAE 논문](https://arxiv.org/abs/1711.00937)
- [DDPM 논문](https://arxiv.org/abs/2006.11239)
- [DDIM 논문](https://arxiv.org/abs/2010.02502)
- [Classifier Guidance 논문](https://arxiv.org/abs/2105.05233)
- [Classifier-free Guidance 논문](https://arxiv.org/abs/2207.12598)
- [LDM 논문](https://arxiv.org/abs/2112.10752)
- [Latent Diffusion Model](https://arxiv.org/abs/2112.10752)
- [Stable Diffusion XL](https://arxiv.org/abs/2307.01952)
- [SDXL Turbo (Adversarial Diffusion Distillation)](https://arxiv.org/abs/2311.17042)




## 5.2 Product Serving
- paper 없음


## 5.3 최적화/경량화
- [Rethinking the Value of Network Pruning](https://arxiv.org/abs/1810.05270)
- [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877)
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks:](https://arxiv.org/abs/1803.03635)
- [Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)
- [Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning](https://arxiv.org/abs/2002.08307)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- [Parameter-Efficient Transfer Learning for NLP ](https://arxiv.org/pdf/1902.00751)
- [Prompt tuning ](https://arxiv.org/pdf/2104.08691)
- [Prefix tuning ](https://arxiv.org/pdf/2101.00190)
- [AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

---

**여기서부터는 논문외의 읽을거리들로 있었던 것들을 정리하였습니다.**
# 6. 📌Further Reading에 있었던 것들.
## 6.1 공통코스
### 6.1.1 Pytorch
- [Introduction to PyTorch — PyTorch Tutorials documentation](https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html)
- [텐서(Tensor) — 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)](https://tutorials.pytorch.kr/beginner/introyt/introyt1_tutorial.html)
- [torch.Tensor — PyTorch documentation](https://pytorch.org/docs/main/tensors.html)
- [부동소수점 - 백과사전](https://ko.wikipedia.org/wiki/%EB%B6%80%EB%8F%99%EC%86%8C%EC%88%98%EC%A0%90)
- [torch.randn — PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn)
- [torch.Tensor — PyTorch documentation](https://pytorch.org/docs/main/tensors.html)
- [GPU와 AI - 네이버 지식백과](https://terms.naver.com/entry.naver?docId=2080143&cid=50305&categoryId=50305)
- [torch.Tensor.view — PyTorch main documentation](https://pytorch.org/docs/main/generated/torch.Tensor.view.html#torch-tensor-view)
- [torch.reshape — PyTorch main documentation](https://pytorch.org/docs/main/generated/torch.reshape.html#torch-reshape)
- [Difference between view, reshape, transpose and permute in PyTorch](https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/)
- [torch.squeeze — PyTorch main documentation](https://pytorch.org/docs/main/generated/torch.squeeze.html#torch-squeeze)
- [Tensor 모양 설명](https://velog.io/@jk01019/broadcastto-repeat-repeatinterleave-view-reshape-expand-expandas-tile-flatten-unsqueeze-squeeze-stack-cat-d1n8ersb)
- [$L_p$ norm](https://en.m.wikipedia.org/wiki/Lp_space)
- [선형회귀](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)
- [경사하강법 학습방법](https://www.youtube.com/watch?v=IHZwWFHWa-w)
- [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)
- [PyTorch DataLoader — PyTorch main documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
- [BCELoss — PyTorch main documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)
- [BCEWithLogitsLoss — PyTorch main documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)
- [CrossEntropyLoss — PyTorch main documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)

### 6.1.2 ML LifeCycle
- [A survey on Image Data Augmentation for Deep Learning ](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0) 
- [Clipping](https://arxiv.org/pdf/1211.5063) 
- [LSTM — PyTorch main documentation ](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) 
- [Attention Is All You Need ](https://arxiv.org/pdf/1706.03762) 
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ](https://arxiv.org/pdf/1810.04805) 
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ](https://arxiv.org/pdf/2010.11929) 


### 6.1.3 EDA & DataViz
- [[Harvard Business Review] Boost Your Team's Data Literacy](https://hbr.org/2020/02/boost-your-teams-data-literacy)
- [[Sequoia Capital] Why Data Science Matters](https://medium.com/sequoia-capital/why-data-science-matters-ee583f785a55)
- [[Sequoia Capital] Role of Data Scientist](https://medium.com/sequoia-capital/five-core-skills-of-a-data-scientist-fc044014fafa)
- [데이터 시각화 교과서 (클라우스 윌케 저)](https://clauswilke.com/dataviz/index.html)
- [Data Viz Project](https://datavizproject.com/)
- [Misleading Data Visualization](https://pressbooks.library.torontomu.ca/criticaldataliteracy/chapter/misleading-data-visualizations/)
- [[The Economists] Mistakes, we've drawn a few](https://medium.economist.com/mistakes-weve-drawn-a-few-8cdd8a42d368)
- [[Google ML Course] Types of Bias](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias?hl=ko)
- [[Github] Awesome Feature Engineering](https://github.com/aikho/awesome-feature-engineering)
- [[서울대 AI 연구원] 다차원 데이터 시각화와 AI (컴퓨터공학부 서진욱 교수)](https://www.youtube.com/watch?v=ymA1spEAd7M)
- [[Distill] How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)
- [Forecasting: Principles & Practice](https://otexts.com/fppkr/arima.html)
- [Hugging Face - NLP Course](https://huggingface.co/learn/nlp-course/ko/chapter1/2?fw=pt)
- [Text Visualization Browser](https://textvis.lnu.se/)
- [[Github] eugeneyan/applied-ml](https://github.com/eugeneyan/applied-ml)
- [[Material Design] Data Visualization](https://m2.material.io/design/communication/data-visualization.html)
- [잘못 사용된 시각화 모음 WTF.viz](https://viz.wtf/)
- [Startup Metrics for Pirates: AARRR! - Dave McClure](https://www.youtube.com/watch?v=irjgfW0BIrw)
- [데이터 시각화, 인지과학을 만나다 ](https://www.yes24.com/Product/Goods/19013968)
- [도널드 노만의 UX 디자인 특강 ](https://www.yes24.com/Product/Goods/59673763)
- [UX/UI의 10가지 심리학 법칙 ](https://product.kyobobook.co.kr/detail/S000212939982)

### 6.1.4 AI 개발 기초
- ["Clean Code: A Handbook of Agile Software Craftsmanship" by Robert C. Martin:](https://www.oreilly.com/library/view/clean-code-a/9780136083238/)
- ["Design Patterns: Elements of Reusable Object-Oriented Software" by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissidesn](https://www.oreilly.com/library/view/design-patterns-elements/0201633612/)
- ["모던 리눅스 교과서" by 마이클 하우센블라스](https://product.kyobobook.co.kr/detail/S000210138053)
- ["The Linux Command Line" by William Shotts:](https://wiki.lib.sun.ac.za/images/c/ca/TLCL-13.07.pdf)
- [Streamlit 공식 문서](https://docs.streamlit.io/)
- [Python 3.x Docs: Virtual Environments and Packages](https://docs.python.org/3/tutorial/venv.html)



## 6.2 CV
### 6.2.1 CV 이론   
- [한겨례 뉴스 "빌게이츠도 감탄한 최예진 교수, 생성형 AI 학습 데이터 공개해야"](https://www.hani.co.kr/arti/economy/economy_general/1128825.html)




### 6.2.2 CV 기초 프로젝트
- [Kaggle Competition](https://www.kaggle.com/competitions)
- [Image file format](https://en.wikipedia.org/wiki/Image_file_format)
- [Multilabel Image Classification Using Deep Learning](https://www.mathworks.com/help/deeplearning/ug/multilabel-image-classification-using-deep-learning.html)
- [Training with Pytorch](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)
- [Pytorch: Automatic Mixed Precision ](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)
- [Nvidia: Mixed-Precision-Training](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)
- [Tensorboard](https://www.tensorflow.org/tensorboard/get_started?hl=ko)
- [WandB](https://wandb.ai/site/ko/)  

### 6.2.3 Object Detection
- [mmdetection github](https://github.com/open-mmlab/mmdetection)
- [detectron2 github](https://github.com/facebookresearch/detectron2)
- [Paperswithcode Object Detection](https://paperswithcode.com/task/object-detection)
- [Kaggle](https://www.kaggle.com/competitions)

### 6.2.4 Data-Centric AI
- [AI 최신 활용 사례](https://www.content.upstage.ai/blog/insight/examples-of-artificial-intelligence)
- [현실 세계에서의 데이터중심 AI](https://ko.upstage.ai/blog/tech/data-centric-ai-in-the-real-world)
- [SynthText in the Wild Dataset](https://www.robots.ox.ac.uk/~vgg/data/scenetext/)
- [Tesseract (Off-the-shelf OCR open source)](https://tesseract-ocr.github.io/)
- [Data annotation에 대한 OpenCV의 blogpost](https://opencv.org/blog/data-annotation/)


### 6.2.5 Semantic Segmentation
- [Segmentation models library](https://github.com/qubvel/segmentation_models)

### 6.2.6 CV Recent Trends
- [Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/)


## 6.3 NLP
### 6.3.1 NLP 이론
- [Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)
- [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)
- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [cs231n Lecture 10: Recurrent Neural Networks](https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)
- [The Exploding and Vanishing Gradients Problem in Time Series](https://medium.com/metaor-artificial-intelligence/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [BertViz: Visualize Attention in NLP Models](https://github.com/jessevig/bertviz)
- [Foundations of NLP Explained Visually: Beam Search, How It Works](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24)
- [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)

  
### 6.3.2 NLP 기초 프로젝트
- [Improving Language Understanding by Generative Pre-Training](https://paperswithcode.com/paper/improving-language-understanding-by)
- [Tokenizer 에 대해](https://huggingface.co/docs/tokenizers/index)
- [Hugging Face Tokenizer 활용하기](https://huggingface.co/docs/tokenizers/quicktour)
- [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary#summary-of-the-tokenizers)
- [Hugging Face Tutorial](https://huggingface.co/learn/nlp-course/chapter1/1)
- [Hugging Face 문서](https://huggingface.co/docs)
- [T-value 와 P-value](https://www.allmath.com/t-critical-value.php)
- [BertViz](https://github.com/jessevig/bertviz)
- [Tune Hyperparameters with Sweeps (wandb.ai)](https://docs.wandb.ai/guides/sweeps/)
- [PEFT (Hugging Face)](https://huggingface.co/PEFT)
- [Parameter와 Hyperparameter의 차이](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)
- [Text classification (Hugging Face)](https://huggingface.co/docs/transformers/tasks/sequence_classification)
- [Image Embedding과 Triplet Loss 간단 설명 (tistory.com)](https://computing-jhson.tistory.com/134)
- [k-최근접 이웃 알고리즘 - 위키백과, 우리 모두의 백과사전 (wikipedia.org)](https://ko.wikipedia.org/wiki/K-최근접_이웃_알고리즘)
- [Embedding Distance | 🦜️🔗 LangChain](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/string/embedding_distance/)
- [BertForSequenceClassification](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification)
- [transformers/src/transformers/models/bert/modeling_bert.py at v4.34.1 · huggingface/transformers (github.com)](https://github.com/huggingface/transformers/blob/v4.34.1/src/transformers/models/bert/modeling_bert.py#L1519)
- [What is Summarization? - Hugging Face](https://huggingface.co/tasks/summarization)
- [BART](https://huggingface.co/docs/transformers/main/en/model_doc/bart#overview)
- [OpenAI GPT2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2#openai-gpt2)
- [Trainer(Hugging Face)](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer)
- [WandB](https://wandb.ai/site/ko/)
- [Model Hub(Hugging Face)](https://huggingface.co/docs/hub/models-the-hub)
- [Model Card (Hugging Face)](https://huggingface.co/docs/huggingface_hub/main/ko/guides/model-cards)
- [Streamlit tutorial](https://docs.streamlit.io/get-started/tutorials/create-an-app)
- [Annotated Model Card Template (Hugging Face)](https://huggingface.co/docs/hub/model-card-annotated#annotated-model-card-template)




### 6.3.3 MRC(Machine Reading Comprehension)
- [문자열 type에 관련된 정리글](https://kunststube.net/encoding/)
- [KorQuAD 데이터 소개 슬라이드](https://www.slideshare.net/slideshow/korquad-introduction/129498665)
- [Naver Engineering: KorQuAD 소개 및 MRC 연구 사례 영상](https://tv.naver.com/v/5564630)
- [SQuAD 데이터셋 둘러보기](https://rajpurkar.github.io/SQuAD-explorer/)
- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)
- [Huggingface datasets](https://huggingface.co/datasets)
- [Introducing BART](https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html)
- [Pyserini BM25 MSmarco documnet retrieval 코드](https://github.com/castorini/pyserini/blob/master/docs/experiments-msmarco-doc.md)
- [Sklearn feature extractor](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
- [Open domain QA tutorial: Dense retrieval](https://github.com/danqi/acl2020-openqa-tutorial/blob/master/slides/part5-dense-retriever-e2e-training.pdf)
- [FAISS blog](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)
- [FAISS github](https://github.com/facebookresearch/faiss)
- [FAISS tutorial](https://github.com/facebookresearch/faiss/tree/main/tutorial/python)
- [Getting started with Faiss](https://www.pinecone.io/learn/series/faiss/faiss-tutorial/)
- [ACL 2020 ODQA tutorial](https://slideslive.com/38931668/t8-opendomain-question-answering)
- [Phrase Retrieval and Beyond](https://princeton-nlp.github.io/phrase-retrieval-and-beyond/)

### 6.3.4 Data-Centric NLP
- [Week 35 - 모델 중심에서 데이터 중심의 AI 개발로](https://jiho-ml.com/weekly-nlp-35/)
- [Crowdsourced Data Collection Benefits & Best Practices ['25]](https://research.aimultiple.com/crowdsourced-data/)
- [KLUE benchmark](https://klue-benchmark.com/)
- [EIGHTH CONFERENCE ON MACHINE TRANSLATION (WMT23)](https://www2.statmt.org/wmt23/)
- [Tokenizers_huggingface](https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt)
- [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary)
- [2023년 1분기 인공지능(AI) 및 자연어처리(NLP) 주요 뉴스](https://www.letr.ai/ko/blog/story-230407)
- [LLM에 환각이 발생하는 원인과 해결 방안](https://moon-walker.medium.com/llm%EC%97%90-halluciation-%ED%99%98%EA%B0%81-%EC%9D%B4-%EB%B0%9C%EC%83%9D%ED%95%98%EB%8A%94-%EC%9B%90%EC%9D%B8%EA%B3%BC-%ED%95%B4%EA%B2%B0%EB%B0%A9%EC%95%88-f18759f0a959)


### 6.3.5 Generative for NLP
- [LLaMA 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
- [LLaMA 구조 설명](https://pub.towardsai.net/llama-explained-a70e71e706e9)
- [LLaMA Github 모델링 코드](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)
- [Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- [What is reinforcement learning from human feedback](https://www.datacamp.com/blog/what-is-reinforcement-learning-from-human-feedback)
- [Needle In A Haystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
- [Jamba 1.5 Open Model Family: The Most Powerful and Efficient Long Context Models](https://www.ai21.com/blog/announcing-jamba-model-family)


### 6.3.6 NLP Recent Trends
- [Prompt-engineering ](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#external-apis)
- [Introduction to LLM Agents](https://developer.nvidia.com/blog/introduction-to-llm-agents/)
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
- [BingAI](https://www.microsoft.com/ko-kr/bing?form=MA13QZ&cs=1873324239)
- [Biases in Large Language Models: Origins, Inventory, and Discussion (Navigli et al., Journal of Data and Information Quality 2023)](https://dl.acm.org/doi/10.1145/3597307)
- [WhatsApp’s AI shows gun-wielding children when prompted with ‘Palestine’](https://www.theguardian.com/technology/2023/nov/02/whatsapps-ai-palestine-kids-gun-gaza-bias-israel)
- [주제별로 알아보는 continual learning](https://tech.scatterlab.co.kr/continual-learning/)
- [SearchGPT 기능](https://openai.com/index/introducing-chatgpt-search/)
- [Reducing Toxicity in Language Models](https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/)
- [Red Teaming Language Models with Language Models (Perez et al., EMNLP 2022)](https://arxiv.org/abs/2202.03286)
- [LangChain](https://www.langchain.com/)
- [LangChain Explained in 13 Minutes](https://www.youtube.com/watch?v=aywZrzNaKjs)

## 6.4 Recsys
### 6.4.1 Recsys 이론
- [중심극한정리, Central Limit Theorem](https://m.blog.naver.com/mykepzzang/220851280035)
- [최대우도법(MLE)](https://angeloyeo.github.io/2020/07/17/MLE.html)
- [가우시안 혼합 모델(Gaussian Mixture Model)](https://untitledtblog.tistory.com/133)
- [KL divergence](https://angeloyeo.github.io/2020/10/27/KL_divergence.html)
- [VI(Variational Inference)](https://velog.io/@gibonki77/Inference-2)
- [Markov Chain Monte Carlo - 공돌이의 수학정리 노트](https://angeloyeo.github.io/2020/09/17/MCMC.html)
- [Influence Function에 대한 이해](https://tootouch.github.io/IML/influential_instances/)
- [Trustworthy Machine Learning (Chapter 3 p.116-p.219)](https://arxiv.org/abs/2310.08215)
- [Python으로 하는 인과추론 : 개념부터 실습까지](https://github.com/CausalInferenceLab/Causal-Inference-with-Python)


### 6.4.2 ML 기초 프로젝트
- [Is there a rule-of-thumb for how to divide a dataset into training and validation sets? - stackoverflow](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio)
- [Data Leakage in The ICML 2013 Whale Challenge](https://www.kaggle.com/competitions/the-icml-2013-whale-challenge-right-whale-redux/discussion/4865)
- [regression-metrics-for-mashine-learning](https://machinelearningmastery.com/regression-metrics-for-machine-learning/)
- [Cross-Validation-Techniques](https://medium.com/geekculture/cross-validation-techniques-33d389897878)
- [Weights & Biases 공식 튜토리얼 문서](https://docs.wandb.ai/tutorials/)
- [Weights & Biases 공식 예제 깃허브 저장소](https://github.com/wandb/examples)
- [The Kaggle Book: Data analysis and machine learning for competitive data science. Packt Publishing Ltd, 2022.](https://github.com/PacktPublishing/The-Kaggle-Book)

### 6.4.3 Competitive DS
- [An Introduction to Statistical Learning](https://www.statlearning.com)
- [Pattern Recognition and Machine Learning](https://link.springer.com/book/9780387310732)
- [Time Series Analysis: Forecasting and Control](https://www.wiley.com/en-us/Time+Series+Analysis%3A+Forecasting+and+Control%2C+5th+Edition-p-9781118675021)
- [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
- [Scikit-learn Documentation](https://scikit-learn.org/stable/index.html)
- [Deep Learning book](https://www.deeplearningbook.org/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Introduction to Data Mining](https://www.pearson.com/en-us/subject-catalog/p/introduction-to-data-mining/P200000003204/9780137506286)
- [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/en/stable/)
- [CatBoost Documentation](https://catboost.ai/docs/en/)
- [Deep Learning for Time Series Forecasting](https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/)
- [Feature Engineering and Selection: A Practical Approach for Predictive Models](https://www.amazon.com/Feature-Engineering-Selection-Practical-Predictive/dp/1138079227)
- [Data Science for Business](https://www.oreilly.com/library/view/data-science-for/9781449374273/)
- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
- [Ensemble Methods: Foundations and Algorithms](https://www.routledge.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/p/book/9781439830031)
- [Automated Machine Learning: Methods, Systems, Challenges](https://link.springer.com/book/10.1007/978-3-030-05318-5)
- [Hyperparameter Optimization: A Practical Guide](https://link.springer.com/chapter/10.1007/978-3-030-05318-5_3)
- [Optuna Documentation](https://optuna.readthedocs.io/en/stable/)

### 6.4.4 RecSys 기초프로젝트
- [[추천 시스템] 성능 평가 방법 - Precision, Recall, NDCG, Hit Rate, MAE, RMSE](https://sungkee-book.tistory.com/11)
- [한국어와 NLTK, Gensim의 만남 @ PyCon Korea 2015](https://www.lucypark.kr/docs/2015-pyconkr/#1)
- [17 types of similarity and dissimilarity measures used in data science.](https://towardsdatascience.com/17-types-of-similarity-and-dissimilarity-measures-used-in-data-science-3eb914d2681)
- [Word2Vec 그리고 추천 시스템의 Item2Vec](https://brunch.co.kr/@goodvc78/16)
- [TOROS N2 - lightweight approximate Nearest Neighbor library](https://www.slideshare.net/ifkakao/toros-n2-lightweight-approximate-nearest-neighbor-library-119540069)
- [ANN-Benchmarks](https://ann-benchmarks.com/index.html)
- [(영상) XGBoost - StatQuest](https://www.youtube.com/playlist?list=PLblh5JKOoLULU0irPgs1SnKO6wqVjKUsQ)
- [(영상) 04-8: Ensemble Learning - LightGBM (앙상블 기법 - LightGBM)](https://www.youtube.com/watch?v=4C8SUZJPlMY)
- [Catboost 주요 개념과 특징 이해하기](https://dailyheumsi.tistory.com/136)
- [(영상) 뭐볼까? : 네이버 AiRS 인공지능 콘텐츠 추천의 진화](https://tv.naver.com/v/16968202)
- [논문 리뷰 A Contextual-Bandit Approach to Personalized News Article Recommendation](https://leehyejin91.github.io/post-contextual_bandit/)
- [RecSys Challenge 2023 Homepage](https://www.recsyschallenge.com/2023/)


### 6.4.5 Movie Rec
- [Paperswithcode(MovieLens)](https://paperswithcode.com/dataset/movielens)
- [01. 추천시스템 이해](https://eda-ai-lab.tistory.com/522)
- [awesome-RecSys](https://github.com/jihoo-kim/awesome-RecSys)
- [Collaborative Denoising Auto-Encoders for Top-N Recommender Systems](https://www.amazon.science/publications/collaborative-denoising-auto-encoders-for-top-n-recommender-systems)
- [Artwork Personalization at Netflix](https://netflixtechblog.com/artwork-personalization-c589f074ad76)
- [Learning a Personalized Homepage](https://netflixtechblog.com/learning-a-personalized-homepage-aa8ec670359a)


### 6.4.6 Recsys Recent Trends
- [(블로그) Enhancing Recommendation Systems with Large Language Models](https://medium.com/microsoftazure/recommendation-systems-enhanced-by-llms-fe1fc8e23a58)

## 6.5 Generative AI
- [LLM 모델 튜닝, 하나의 GPU로 가능할까? Parameter Efficient Fine-Tuning(PEFT)을 소개합니다!](https://devocean.sk.com/blog/techBoardDetail.do?ID=164779&boardType=techBlog)
- [Alpaca 홈페이지](https://crfm.stanford.edu/2023/03/13/alpaca.html)


## 6.6 Product Serving
- [Uber의 Michelangelo 플랫폼에 관한 글로, 대규모 머신러닝 모델 서빙 방법에 대한 통찰을 살펴볼 수 있습니다.](https://www.uber.com/en-KR/blog/scaling-michelangelo/)
- [Machine learning is going real-time](https://huyenchip.com/2020/12/27/real-time-machine-learning.html)
- [쏘카의 Airflow 구축기](https://tech.socarcorp.kr/data/2021/06/01/data-engineering-with-airflow.html)
- [쏘카의 Airflow 구축기2](https://tech.socarcorp.kr/data/2022/11/09/advanced-airflow-for-databiz.html)
- [버킷플레이스 Airflow 도입기](https://www.bucketplace.com/post/2021-04-13-%EB%B2%84%ED%82%B7%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-airflow-%EB%8F%84%EC%9E%85%EA%B8%B0/)
- [라인 엔지니어링 Airflow on Kubernetes](https://engineering.linecorp.com/ko/blog/data-engineering-with-airflow-k8s-1)
- [Day1, 2-2. 그런 REST API로 괜찮은가](https://www.youtube.com/watch?v=RP_f5dMoHFc)
- [HTTP response status codes](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Advanced User Guide for FastAPI](https://fastapi.tiangolo.com/advanced/)
- [Awesome-fastapi](https://github.com/mjhea0/awesome-fastapi)
- [도커(Docker) 입문편: 컨테이너 기초부터 서버 배포까지](https://www.44bits.io/ko/post/easy-deploy-with-docker)
- [GCP 공식 문서](https://cloud.google.com/docs?_gl=1*1qvbnv0*_up*MQ..&gclid=CjwKCAiA9bq6BhAKEiwAH6bqoH2F6QSWY2FuBmbvo00dn9Q17vhlVZWt-IHvBB4JCmDoHUTjymDqQhoCnygQAvD_BwE&gclsrc=aw.ds&hl=ko)
- [AWS를 이용한 MLOps 구축 사례 살펴보기](https://aws.amazon.com/ko/blogs/tech/aws-mlops-use-case/)
- [MLOps Principles - Automation](https://ml-ops.org/content/mlops-principles#automation)
- [Experiments Tracking](https://ml-ops.org/content/mlops-principles#reproducibility)
- [Reproducibility](https://ml-ops.org/content/mlops-principles#reproducibility)
- [Data Engineer Jobs](https://www.linkedin.com/jobs/data-engineer-jobs/)

## 6.7 최적화/경량화
- [ Softmax Temperature](https://medium.com/@harshit158/softmax-temperature-5492e4007f71)
- [Lecture 10 - Knowledge Distillation | MIT 6.S965](https://www.youtube.com/watch?v=tT9Lnt6stwA)
- [ A brief overview of Imitation Learning](https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c)
- [Dynamic Quantization](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html)
- [딥러닝의 Quantization (양자화)와 Quantization Aware Training:](https://gaussian37.github.io/dl-concept-quantization/#qat-quantization-aware-training-%EB%B0%A9%EB%B2%95-1)
- [Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)
- [Brief Overview of Parallelism Strategies](https://afmck.in/posts/2023-02-26-parallelism/)
- [Data Parallelism on CNN:](https://siboehm.com/articles/22/data-parallel-training)
- [Pipeline Parallelism Algorithm](https://siboehm.com/articles/22/pipeline-parallel-training)
- [단일 머신을 사용한 모델 병렬화 모범 사례](https://tutorials.pytorch.kr/intermediate/model_parallel_tutorial.html)

# 7. 📌Contribution 하는 방법

## 7.1 Fork를 한다.
- 원본 저장소를 내 계정으로 복사(Fork)

## 7.2 PR를 보낸다.
- Fork한 저장소에서 변경 내용을 작업한 뒤 Pull Request(Pull Request)를 생성

## 7.3 Approve를 받으면 Merge!
- 리뷰 후 승인을 받으면 원본 저장소에 변경사항이 병합(Merge)

>이 문서는 계속 업데이트 예정입니다.
궁금한 사항이나 제안이 있다면 Issue 혹은 Pull Request로 알려주세요!
감사합니다.

## 8. 📌Updates (Changelog) - finished at 25-02-21🔥

| 날짜       | 변경 사항                                                                                                         |
|------------|-------------------------------------------------------------------------------------------------------------------|
| 2025-01-01 | 🎨 **CV 업데이트**<br/>- CV 섹션 논문 목록을 추가하고 깔끔하게 정렬했습니다.                                        |
| 2025-01-02 | ✨ **NLP 이론 섹션 업데이트**<br/>- NLP 이론 파트에 주요 논문 및 레퍼런스를 추가했습니다.                           |
| 2025-01-04 | 🎨 **CV Recent Trends 섹션 업데이트**<br/>- CV recent trends 섹션에 최근 논문 목록을 추가했습니다. |
| 2025-01-13 | ✨ **NLP 기초 프로젝트 섹션 업데이트**<br/>- NLP 기초 프로젝트 파트에 주요 논문 및 레퍼런스를 추가했습니다.          |
| 2025-01-13 | 🌱 **Recsys 이론 및 ML 기초 프로젝트 섹션 업데이트**<br/>- Recsys 기초 프로젝트 파트에 주요 논문 및 레퍼런스를 추가했습니다.      |
| 2025-01-14 | ✨ **NLP MRC & Data-Centric 섹션 업데이트**<br/>- NLP 기초 프로젝트 파트에 주요 논문 및 레퍼런스를 추가했습니다.          |
| 2025-01-14 | 🌱 **Recsys Competitive DS & Recsys 기초프로젝트 섹션 업데이트**<br/>- Recsys 기초 프로젝트 파트에 주요 논문 및 레퍼런스를 추가했습니다.      |
| 2025-01-21 | ✨ **NLP Generation & Recent  섹션 업데이트**<br/>- NLP  파트에 주요 논문 및 레퍼런스를 추가했습니다.          |
| 2025-02-21 | 🔥 **모든 paper를 추가하였습니다** |


## 9. 📌Star History
## Star History
[![Star History Chart](https://api.star-history.com/svg?repos=2JAE22/NaverAIBoostCamp_7th_paper&type=Date)](https://star-history.com/#2JAE22/NaverAIBoostCamp_7th_paper&Date)

